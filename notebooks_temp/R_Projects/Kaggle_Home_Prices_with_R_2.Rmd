---
title: "My R code from the Level 2 exercises of Kaggle's Learn Machine Learning series"
author: "Michael Skolnik"
subtitle: ""
date: "2018-05-04"
slug: "home-prices-r2"
tags: ["R", "machine learning"]
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## *Learn Maching Learning* series on Kaggle in R

This is my R code for the first two sections of the level 2 part of the *Learn Machine Learning* series on Kaggle. I've already done the Python one, which is on Kaggle located [here](https://www.kaggle.com/learn/machine-learning). The data used is from the [*Home Prices: Advanced Regression Techniques*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition.

I had planned on doing all of level 2 but that was more difficult than I expected. One reason for that is the Python series told me what packages to use and gave me an outline of steps to follow. Since I'm doing this in R with no tutorial to follow, I had to research and decide which packages to use and steps to follow. That slowed me down but I definitely learned a lot. I also had problems with Jupyter and the R kernel, so I did this in RStudio as an R Markdown file.

### Load and install packages and load the data
I learned since the last time that I need to do an if-else statement, not just if, when checking for packages before loading them.

```{r}
# Install and load packages
if (!require("randomForest")) {
  install.packages("randomForest", repos="http://cran.rstudio.com/")
  library(randomForest, lib.loc="~/R/win-library/3.5")
} else {
  library(randomForest, lib.loc="~/R/win-library/3.5")
}

if (!require("dplyr")) {
  install.packages("dplyr", repos="http://cran.rstudio.com/")
  library(dplyr, lib.loc="~/R/win-library/3.5")
} else {
  library(dplyr, lib.loc="~/R/win-library/3.5")
}

if (!require("caTools")) {
  install.packages("caTools", repos="http://cran.rstudio.com/")
  library(caTools, lib.loc="~/R/win-library/3.5")
} else {
  library(caTools, lib.loc="~/R/win-library/3.5")
}

if (!require("rpart")) {
  install.packages("rpart", repos="http://cran.rstudio.com/")
  library(rpart, lib.loc="~/R/win-library/3.5")
} else {
  library(rpart, lib.loc="~/R/win-library/3.5")
}

# Save filepath to variable
training_data_filepath <- "C:/Development/Kaggle/House Prices - Advanced Regression Techniques/train.csv"

# Import data
dataset <- read.csv(training_data_filepath)
```

### Split the data set into training and test

This is the same as before.
```{r}
# Split data into training and validation data, for both predictors and target.
set.seed(42)
split <- sample.split(dataset, SplitRatio=0.7)  # for training data
training_set <- subset(dataset, split==TRUE)
test_set <- subset(dataset, split==FALSE)
```

### Select only the numeric predictors and then impute missing data
```{r}
# Create the training and tests dataframe with only numeric predictors
nums <- unlist(lapply(training_set, is.numeric))
training_set_nums <- training_set[, nums]
test_set_nums <- test_set[, nums]

# Show the number of NAs in each field for the training set
apply(training_set_nums, 2, function(x) {sum(is.na(x))})

# Show the number of NAs in each field for the test set
apply(test_set_nums, 2, function(x) {sum(is.na(x))})

# Impute missing data using rfImpute
training_set_nums_impute <- rfImpute(SalePrice ~ ., training_set_nums)
test_set_nums_impute <- rfImpute(SalePrice ~ ., test_set_nums)
```

### Create the predictor and target variables from the imputed data

Then get the MAEs for multiple ntree values.
```{r}
# Create the predictor variable
X <- subset(training_set_nums_impute, select = -c(Id, SalePrice))

# Select the target variable and call it y
y <- training_set_nums_impute$SalePrice

# Create a function to calculate the Mean Absolute Error
mae <- function(error)
{
  mean(abs(error))
}

# Create a function to get the MAE of a random forest
getMae_forest <- function(X, y, test_data, n) {
  set.seed(42)
  regressor <- randomForest(x=X, y=y, ntree=n)
  y_prediction <- predict(regressor, newdata=test_data)
  y_test <- test_data$SalePrice
  error <- (y_prediction - y_test)
  print(paste("ntree of ", n, " has an MAE of ", mae(error), sep=""))
}

# Loop through multiple ntree values
ntrees = c(1, 5, 10, 30, 50, 100, 500, 1000, 5000)

for (i in ntrees) {
  getMae_forest(X, y, test_set_nums_impute, i)
}
```
ntree of 500 has lowest MAE

### Encoding categorical variables

Random forest in R converts categorical variables to dummy for you.
```{r}
# Split data into training and validation data, for both predictors and target.
set.seed(42)
split <- sample.split(dataset, SplitRatio=0.7)  # for training data
training_set <- subset(dataset, split==TRUE)
test_set <- subset(dataset, split==FALSE)

# Impute missing data using rfImpute
training_set_impute <- rfImpute(SalePrice ~ ., training_set)
test_set_impute <- rfImpute(SalePrice ~ ., test_set)

# Create the predictor variable
X <- subset(training_set_impute, select = -c(Id, SalePrice))

# Select the target variable and call it y
y <- training_set_impute$SalePrice

# Loop through multiple ntree values
ntrees = c(1, 5, 10, 30, 50, 100, 500, 1000, 5000)

for (i in ntrees) {
  getMae_forest(X, y, test_set_impute, i)
}
```
ntree of 5000 has the lowest MAE but that could be because of a bias with random forests. A quote from Wikipediaâ€™s article on random forests (in the [variable importance section](https://en.wikipedia.org/wiki/Random_forest#Variable_importance)):
  
  "For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels."

That tells me we are including too many and going forward I'll need to limit them by using numeric variables only or some mix of the two. I'll have to do some research to determine the best way to determine a mix then.
