---
title: "My R code from the Level 1 exercises of Kaggle's Learn Machine Learning series"
author: "Michael Skolnik"
subtitle: ""
date: "2018-04-19"
slug: "home-prices-r1"
tags: ["R", "machine learning"]
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## *Learn Maching Learning* series on Kaggle in R

This is my R code for the level 1 part of the *Learn Machine Learning* series on Kaggle. I've already done the Python one, which is on Kaggle located [here](https://www.kaggle.com/learn/machine-learning). The data used is from the [*Home Prices: Advanced Regression Techniques*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition.

Originally I had planned on doing both level 1 and level 2 at the same time, but I encountered some issues with my R install and I got busier than expected. I'm publishing level 1 now then and while I've already started the level 2 part, I'll just publish it a little later.

### Load and install packages and load the data

```{r}
# Install and load packages
if (!require("randomForest")) {
  install.packages("randomForest", repos="http://cran.rstudio.com/")
  library(randomForest)
}

if (!require("dplyr")) {
  install.packages("dplyr", repos="http://cran.rstudio.com/")
  library(dplyr)
}

if (!require("caTools")) {
  install.packages("caTools", repos="http://cran.rstudio.com/")
  library(caTools)
}

if (!require("rpart")) {
  install.packages("rpart", repos="http://cran.rstudio.com/")
  library(rpart)
}

# Save filepath to variable
training_data_filepath <- "C:/Development/Kaggle/House Prices - Advanced Regression Techniques/train.csv"

# Import data
dataset <- read.csv(training_data_filepath)
```

### View some stats about the data

```{r}

# View some stats and information about the data
summary(dataset)
```

### Split the data set into training and test, then create the predictor and target variables
```{r}
# Split data into training and validation data, for both predictors and target.
set.seed(42)
split <- sample.split(dataset, SplitRatio=0.7)  # for training data
training_set <- subset(dataset, split==TRUE)
test_set <- subset(dataset, split==FALSE)

# Create the training and tests dataframe with the initial predictors
predictors <- c("LotArea", "YearBuilt", "X1stFlrSF", "X2ndFlrSF",
                "FullBath", "BedroomAbvGr", "TotRmsAbvGrd", "SalePrice")
training_set <- training_set %>%
  select(predictors)
test_set <- test_set %>%
  select(predictors)

# Create the predictor variable
X <- training_set %>%
  select(-SalePrice)

# Select the target variable and call it y
y <- training_set$SalePrice
```

### Predict values with a Decision Tree using rpart

```{r}
# Fitting Decision Tree to the training data
formula=SalePrice ~ .

regressor <- rpart(formula=formula, data=training_set,
                   control=rpart.control(cp=.01))

y_pred <- predict(regressor, test_set)

# View a summary of the predicted values
summary(y_pred)
```

### Create a function to get the Mean Absolute Error (or MAE)

```{r}
# Calculating the Mean Absolute Error
mae <- function(error)
{
  mean(abs(error))
}

# Get the MAE
y_test <- test_set$SalePrice
error <- (y_test - y_pred)
mae(error)
```

### Create a function to compare the MAE for different cp values

``` {r}
# Create the function
getMae_rpart <- function(formula, training_data, test_data, n) {
  set.seed(42)
  regressor_rpart <- rpart(formula=formula, data=training_data,
                    control=rpart.control(cp=n))
  y_prediction <- predict(regressor_rpart, newdata=test_data)
  y_test <- test_data$SalePrice
  error <- (y_test - y_prediction)
  print(paste("cp of ", n, " has an MAE of ", mae(error), sep=""))
}
```
Set up the formula variable and cp values, then loop through the values and call the function.
```{r}
# Set the formula variable
formula <- SalePrice ~ .

# Loop through multiple ntree values
cps <- c(.5, .1, .05, .02, .01, .005, .003, .001, .0005, .0001)

for (i in cps) {
  getMae_rpart(formula, training_set, test_set, i)
}
```
MAE continues to decrease as the cp decreases.

### Predict values with a Random Forest
```{r}
# Fitting Random Forest Regression to the dataset
regressor <- randomForest(x=X, y=y, ntree=100)

# Predicting a new result
y_pred <- predict(regressor, newdata=test_set)

# Get the MAE
y_test <- test_set$SalePrice
error <- (y_pred - y_test)
mae(error)
```
### Create a function to compare the MAE for different ntree values
```{r}
# Create the function
getMae_forest <- function(X, y, test_data, n) {
  set.seed(42)
  regressor <- randomForest(x=X, y=y, ntree=n)
  y_prediction <- predict(regressor, newdata=test_data)
  y_test <- test_data$SalePrice
  error <- (y_prediction - y_test)
  print(paste("ntree of ", n, " has an MAE of ", mae(error), sep=""))
}

# Loop through multiple ntree values
ntrees = c(1, 5, 10, 30, 50, 100, 500, 1000, 5000)

for (i in ntrees) {
  getMae_forest(X, y, test_set, i)
}
```
ntree of 1000 has the lowest MAE.

That's all for this post. The more I use R, the more I like it. Python and R both have their advantages though.

Hopefully the second part doesn't take me nearly as long. Until then!