{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 *Learn Maching Learning* series on Kaggle\n",
    "I went through the level 1 *Learn Machine Learning* series on Kaggle using Python (https://www.kaggle.com/learn/machine-learning). The data used is from the [*Home Prices: Advanced Regression Techniques*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition.\n",
    "\n",
    "This post will show the section name, my code from the corresponding section for the instructions under **Your Turn**, and some brief notes on what is taught in each section. You should go to the links to learn and also do yourself as I found this very helpful. Even if you've taken other machine learning courses as I have, this is a good refresher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2\n",
    "[Starting Your ML Project](https://www.kaggle.com/dansbecker/starting-your-ml-project)\n",
    "\n",
    "This section has you load the data and set up the computing environment for the project. You also view summary statistics and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
      "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
      "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
      "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
      "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
      "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
      "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
      "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
      "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
      "\n",
      "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  \\\n",
      "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000   \n",
      "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726   \n",
      "std       1.112799    30.202904     20.645407   181.066207   456.098091   \n",
      "min       1.000000  1872.000000   1950.000000     0.000000     0.000000   \n",
      "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000   \n",
      "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000   \n",
      "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000   \n",
      "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000   \n",
      "\n",
      "           ...         WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
      "count      ...        1460.000000  1460.000000    1460.000000  1460.000000   \n",
      "mean       ...          94.244521    46.660274      21.954110     3.409589   \n",
      "std        ...         125.338794    66.256028      61.119149    29.317331   \n",
      "min        ...           0.000000     0.000000       0.000000     0.000000   \n",
      "25%        ...           0.000000     0.000000       0.000000     0.000000   \n",
      "50%        ...           0.000000    25.000000       0.000000     0.000000   \n",
      "75%        ...         168.000000    68.000000       0.000000     0.000000   \n",
      "max        ...         857.000000   547.000000     552.000000   508.000000   \n",
      "\n",
      "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \\\n",
      "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   \n",
      "mean     15.060959     2.758904     43.489041     6.321918  2007.815753   \n",
      "std      55.757415    40.177307    496.123024     2.703626     1.328095   \n",
      "min       0.000000     0.000000      0.000000     1.000000  2006.000000   \n",
      "25%       0.000000     0.000000      0.000000     5.000000  2007.000000   \n",
      "50%       0.000000     0.000000      0.000000     6.000000  2008.000000   \n",
      "75%       0.000000     0.000000      0.000000     8.000000  2009.000000   \n",
      "max     480.000000   738.000000  15500.000000    12.000000  2010.000000   \n",
      "\n",
      "           SalePrice  \n",
      "count    1460.000000  \n",
      "mean   180921.195890  \n",
      "std     79442.502883  \n",
      "min     34900.000000  \n",
      "25%    129975.000000  \n",
      "50%    163000.000000  \n",
      "75%    214000.000000  \n",
      "max    755000.000000  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save filepath to variable\n",
    "training_data_filepath = \"C:/Development/Kaggle/House Prices - Advanced \\\n",
    "Regression Techniques/train.csv\"\n",
    "\n",
    "# Read the data and store in a dataframe called training_set\n",
    "training_set = pd.read_csv(training_data_filepath)\n",
    "\n",
    "# Print a summary of the data in training_set\n",
    "print(training_set.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
      "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
      "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
      "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
      "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
      "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
      "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
      "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
      "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
      "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
      "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
      "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
      "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
      "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
      "       'SaleCondition', 'SalePrice'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the columns in training_set\n",
    "print(training_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3\n",
    "[Selecting and Filtering in Pandas](https://www.kaggle.com/dansbecker/selecting-and-filtering-in-pandas)\n",
    "\n",
    "This section has you use pandas to select the data you want to use, which allows you to get the data ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    208500\n",
      "1    181500\n",
      "2    223500\n",
      "3    140000\n",
      "4    250000\n",
      "Name: SalePrice, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Store the series of prices separately as training_price_data\n",
    "training_price_data = training_set.SalePrice\n",
    "\n",
    "# Print the first 5 records\n",
    "print(training_price_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             LotArea    YearBuilt\n",
      "count    1460.000000  1460.000000\n",
      "mean    10516.828082  1971.267808\n",
      "std      9981.264932    30.202904\n",
      "min      1300.000000  1872.000000\n",
      "25%      7553.500000  1954.000000\n",
      "50%      9478.500000  1973.000000\n",
      "75%     11601.500000  2000.000000\n",
      "max    215245.000000  2010.000000\n"
     ]
    }
   ],
   "source": [
    "# Create a list with the columns I am interested in\n",
    "columns_of_interest = [\"LotArea\", \"YearBuilt\"]\n",
    "\n",
    "# Create a dataframe with just those columns\n",
    "training_two_columns = training_set[columns_of_interest]\n",
    "\n",
    "# Print a summary of the training_two_columns dataframe\n",
    "print(training_two_columns.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4\n",
    "[Your First Scikit-Learn Model](https://www.kaggle.com/dansbecker/your-first-scikit-learn-model)\n",
    "\n",
    "Building your first model! Spoiler: it's a Decision Tree model. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target variable and call it y\n",
    "y = training_set.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the predictor variables\n",
    "predictors = [\"LotArea\", \"YearBuilt\", \"1stFlrSF\", \"2ndFlrSF\", \"FullBath\",\n",
    "              \"BedroomAbvGr\", \"TotRmsAbvGrd\"]\n",
    "\n",
    "# Create a new dataframe with the predictors list\n",
    "X = training_set[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define the first model\n",
    "tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit model\n",
    "tree_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for the first 10 houses\n",
      "   LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
      "0     8450       2003       856       854         2             3   \n",
      "1     9600       1976      1262         0         2             3   \n",
      "2    11250       2001       920       866         2             3   \n",
      "3     9550       1915       961       756         1             3   \n",
      "4    14260       2000      1145      1053         2             4   \n",
      "5    14115       1993       796       566         1             1   \n",
      "6    10084       2004      1694         0         2             3   \n",
      "7    10382       1973      1107       983         2             3   \n",
      "8     6120       1931      1022       752         2             2   \n",
      "9     7420       1939      1077         0         1             2   \n",
      "\n",
      "   TotRmsAbvGrd  \n",
      "0             8  \n",
      "1             6  \n",
      "2             6  \n",
      "3             7  \n",
      "4             9  \n",
      "5             5  \n",
      "6             7  \n",
      "7             7  \n",
      "8             8  \n",
      "9             5  \n",
      "The predictions are:\n",
      "[208500. 181500. 223500. 140000. 250000. 143000. 307000. 200000. 129900.\n",
      " 118000.]\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions\n",
    "print(\"Making predictions for the first 10 houses\")\n",
    "print(X.head(n=10))\n",
    "print(\"The predictions are:\")\n",
    "print(tree_model.predict(X.head(n=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5\n",
    "[Model Validation](https://www.kaggle.com/dansbecker/model-validation)\n",
    "\n",
    "This section introduces model validation to measure the performance of the model. You also learn about \"in-sample\" scores and why you should split your data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30855.94794520548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation data, for both predictors and\n",
    "# target.\n",
    "# The split is based on a random number generator. Supplying a numeric value\n",
    "# to the random_state argument guarantees we get the same split every time we\n",
    "# run this script. It can be any number; I'm choosing 42.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit model\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted prices on validation data\n",
    "predictions_val = tree_model.predict(X_val)\n",
    "print(mean_absolute_error(y_val, predictions_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6\n",
    "[Underfitting, Overfitting, and Model Optimization](https://www.kaggle.com/dansbecker/underfitting-overfitting-and-model-optimization)\n",
    "\n",
    "In this section you learn about underfitting, overfitting, and optimizing your model. The *max_leaf_nodes* argument is used to provide a very sensible way to control overfitting vs underfitting in Decision Tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a utility function to help compare MAE scores from differevalues for\n",
    "# *max_leaf_nodes*.\n",
    "def get_mae(max_leaf_nodes, predictors_train, val_predictors, targ_train,\n",
    "            targ_val):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,\n",
    "                                  random_state=42)\n",
    "    model.fit(predictors_train, targ_train)\n",
    "    preds_val = model.predict(val_predictors)\n",
    "    mae = mean_absolute_error(targ_val, preds_val)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5 \t\t Mean Absolute Error: 35244.94032482636\n",
      "Max leaf nodes: 10 \t\t Mean Absolute Error: 31256.15612179911\n",
      "Max leaf nodes: 25 \t\t Mean Absolute Error: 29611.012298361497\n",
      "Max leaf nodes: 50 \t\t Mean Absolute Error: 27232.09960472095\n",
      "Max leaf nodes: 100 \t\t Mean Absolute Error: 27021.244092878136\n",
      "Max leaf nodes: 200 \t\t Mean Absolute Error: 29015.822642629737\n",
      "Max leaf nodes: 500 \t\t Mean Absolute Error: 31450.856430996708\n",
      "Max leaf nodes: 1000 \t\t Mean Absolute Error: 31717.233789954334\n",
      "Max leaf nodes: 5000 \t\t Mean Absolute Error: 31724.594520547944\n"
     ]
    }
   ],
   "source": [
    "# Loop through a list of max leaf nodes and print the MAE of each\n",
    "for max_leaf_nodes in [5, 10, 25, 50, 100, 200, 500, 1000, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, X_train, X_val, y_train, y_val)\n",
    "    print(\"Max leaf nodes: {0} \\t\\t Mean Absolute Error: {1}\".\n",
    "          format(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 7\n",
    "[Random Forests](https://www.kaggle.com/dansbecker/random-forests)\n",
    "\n",
    "This section has use a Random Forest model and you can compare the results to the Decision Tree one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22458.350528375733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the second model, a Random Forest\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_model.fit(X_train, y_train)\n",
    "forest_preds = forest_model.predict(X_val)\n",
    "print(mean_absolute_error(y_val, forest_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting all the code in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Save filepath to variable\n",
    "training_data_filepath = \"C:/Development/Kaggle/House Prices - Advanced \\\n",
    "Regression Techniques/train.csv\"\n",
    "\n",
    "# Read the data and store in a dataframe called training_set\n",
    "training_set = pd.read_csv(training_data_filepath)\n",
    "\n",
    "# Print a summary of the data in training_set\n",
    "print(training_set.describe())\n",
    "\n",
    "# Print the columns in training_set\n",
    "print(training_set.columns)\n",
    "\n",
    "# Store the series of prices separately as training_price_data\n",
    "training_price_data = training_set.SalePrice\n",
    "\n",
    "# Print the first 5 records\n",
    "print(training_price_data.head())\n",
    "\n",
    "# Create a list with the columns I am interested in\n",
    "columns_of_interest = [\"LotArea\", \"YearBuilt\"]\n",
    "\n",
    "# Create a dataframe with just those columns\n",
    "training_two_columns = training_set[columns_of_interest]\n",
    "\n",
    "# Print a summary of the training_two_columns dataframe\n",
    "print(training_two_columns.describe())\n",
    "\n",
    "# Select the target variable and call it y\n",
    "y = training_set.SalePrice\n",
    "\n",
    "# Create a list of the predictor variables\n",
    "predictors = [\"LotArea\", \"YearBuilt\", \"1stFlrSF\", \"2ndFlrSF\", \"FullBath\",\n",
    "              \"BedroomAbvGr\", \"TotRmsAbvGrd\"]\n",
    "\n",
    "# Create a new dataframe with the predictors list\n",
    "X = training_set[predictors]\n",
    "\n",
    "# Define the first model, a Decision Tree\n",
    "tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit model\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "# Make some predictions\n",
    "print(\"Making predictions for the first 10 houses\")\n",
    "print(X.head(n=10))\n",
    "print(\"The predictions are:\")\n",
    "print(tree_model.predict(X.head(n=10)))\n",
    "\n",
    "# Split data into training and validation data, for both predictors and\n",
    "# target.\n",
    "# The split is based on a random number generator. Supplying a numeric value\n",
    "# to the random_state argument guarantees we get the same split every time we\n",
    "# run this script. It can be any number; I'm choosing 42.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit model\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted prices on validation data\n",
    "predictions_val = tree_model.predict(X_val)\n",
    "print(mean_absolute_error(y_val, predictions_val))\n",
    "\n",
    "# Create a utility function to help compare MAE scores from differevalues for\n",
    "# *max_leaf_nodes*.\n",
    "def get_mae(max_leaf_nodes, predictors_train, val_predictors, targ_train,\n",
    "            targ_val):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,\n",
    "                                  random_state=42)\n",
    "    model.fit(predictors_train, targ_train)\n",
    "    preds_val = model.predict(val_predictors)\n",
    "    mae = mean_absolute_error(targ_val, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "\n",
    "# Loop through a list of max leaf nodes and print the MAE of each\n",
    "for max_leaf_nodes in [5, 10, 25, 50, 100, 200, 500, 1000, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, X_train, X_val, y_train, y_val)\n",
    "    print(\"Max leaf nodes: {0} \\t\\t Mean Absolute Error: {1}\".\n",
    "          format(max_leaf_nodes, my_mae))\n",
    "\n",
    "# Create the second model, a Random Forest\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_model.fit(X_train, y_train)\n",
    "forest_preds = forest_model.predict(X_val)\n",
    "print(mean_absolute_error(y_val, forest_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next post will be the level 2 part of the series and after that I'm going to do it in R. I'm hoping to have level 2 up in a few days, a week at most."
   ]
  }
 ],
 "metadata": {
  "front-matter": {
   "date": "2018-03-25",
   "slug": "home-prices-python",
   "subtitle": "",
   "title": "My work on the data from Level 1 of Kaggle's Learn Maching Learning series"
  },
  "hugo-jupyter": {
   "render-to": "content/post/2018/03/25/"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
