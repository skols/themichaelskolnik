{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2 *Learn Maching Learning* series on Kaggle\n",
    "This is the level 2 part of the *Learn Machine Learning* series on Kaggle using Python (https://www.kaggle.com/learn/machine-learning). The data used is from the [*Home Prices: Advanced Regression Techniques*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition.\n",
    "\n",
    "Like the post for level 1, this post will show the section name, my code from the corresponding section for the instructions under **Your Turn**, and some brief notes on what is taught in each section.\n",
    "\n",
    "First I'll run the necessary code from before and add a new function, score_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Save filepath to variable\n",
    "training_data_filepath = \"C:/Development/Kaggle/House Prices - Advanced \\\n",
    "Regression Techniques/train.csv\"\n",
    "\n",
    "# Read the data and store in a dataframe called training_set\n",
    "training_set = pd.read_csv(training_data_filepath)\n",
    "\n",
    "# Select the target variable and call it y\n",
    "y = training_set.SalePrice\n",
    "\n",
    "# Create the dataframe with only numeric predictors, dropping Id and SalePrice\n",
    "X = training_set.drop([\"Id\", \"SalePrice\"], axis=1)\\\n",
    "        .select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "# Split data into training and validation data, for both predictors and\n",
    "# target.\n",
    "# The split is based on a random number generator. Supplying a numeric value\n",
    "# to the random_state argument guarantees we get the same split every time we\n",
    "# run this script. It can be any number; I'm choosing 42.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                 train_size=0.7,\n",
    "                                                 test_size=0.3)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1\n",
    "[Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values)\n",
    "\n",
    "This section teaches multiple approaches for dealing with missing data fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSSubClass         0\n",
      "LotFrontage      259\n",
      "LotArea            0\n",
      "OverallQual        0\n",
      "OverallCond        0\n",
      "YearBuilt          0\n",
      "YearRemodAdd       0\n",
      "MasVnrArea         8\n",
      "BsmtFinSF1         0\n",
      "BsmtFinSF2         0\n",
      "BsmtUnfSF          0\n",
      "TotalBsmtSF        0\n",
      "1stFlrSF           0\n",
      "2ndFlrSF           0\n",
      "LowQualFinSF       0\n",
      "GrLivArea          0\n",
      "BsmtFullBath       0\n",
      "BsmtHalfBath       0\n",
      "FullBath           0\n",
      "HalfBath           0\n",
      "BedroomAbvGr       0\n",
      "KitchenAbvGr       0\n",
      "TotRmsAbvGrd       0\n",
      "Fireplaces         0\n",
      "GarageYrBlt       81\n",
      "GarageCars         0\n",
      "GarageArea         0\n",
      "WoodDeckSF         0\n",
      "OpenPorchSF        0\n",
      "EnclosedPorch      0\n",
      "3SsnPorch          0\n",
      "ScreenPorch        0\n",
      "PoolArea           0\n",
      "MiscVal            0\n",
      "MoSold             0\n",
      "YrSold             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Detect which columns have missing values\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from dropping columns with missing values:\n",
      "19427.964155251142\n"
     ]
    }
   ],
   "source": [
    "# Get model score from dropping columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test = X_test.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"Mean Absolute Error from dropping columns with missing values:\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation:\n",
      "19439.337519025874\n"
     ]
    }
   ],
   "source": [
    "# Get model score from Imputation\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "\n",
    "# \"fit_transform\" is the training step. It \"learns\" based upon the training set data.\n",
    "# \"transform\" uses the newly trained model to make predictions on the \"test set\"\n",
    "# (a.k.a. \"validation set\" in the the first tutorial).\n",
    "\n",
    "print(\"Mean Absolute Error from Imputation:\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation while tracking what was imputed:\n",
      "18996.393607305938\n"
     ]
    }
   ],
   "source": [
    "# Get model score from Imputation with extra columns showing what was imputed\n",
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "cols_with_missing = (col for col in X_train.columns\n",
    "                    if X_train[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + \"_was_missing\"] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + \"_was_missing\"] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"Mean Absolute Error from Imputation while tracking what was imputed:\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2\n",
    "[Using Categorical Data with One Hot Encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding)\n",
    "\n",
    "In this section you learn how to handle categorical data by using one hot encoding, which creates new columns for each value in the categorical field. The new columns will have either a 1 or 0 in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error with Imputation and One Hot Encoding:\n",
      "18186.471917808216\n"
     ]
    }
   ],
   "source": [
    "# Using cardinality as a way to select categorical data. \"cardinality\" means\n",
    "# the number of unique values in a column.\n",
    "candidate_train_predictors = training_set.drop([\"Id\", \"SalePrice\"], axis=1)\n",
    "\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors if\n",
    "                       candidate_train_predictors[cname].nunique() < 10 and\n",
    "                       candidate_train_predictors[cname].dtype == \"object\"]\n",
    "numeric_cols = [cname for cname in candidate_train_predictors if\n",
    "               candidate_train_predictors[cname].dtype in\n",
    "                [\"int64\", \"float64\"]]\n",
    "\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "X = candidate_train_predictors[my_cols]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size=0.3)\n",
    "\n",
    "# Using one hot encoding the categorical variables\n",
    "X_train_one_hot_encoded = pd.get_dummies(X_train)\n",
    "X_test_one_hot_encoded = pd.get_dummies(X_test)\n",
    "\n",
    "# Make sure the columns show up in the same order by using the align method\n",
    "# \"join='inner'\" is like an inner join in SQL, keeping only the columns in\n",
    "# both datasets\n",
    "X_train_final, X_test_final = X_train_one_hot_encoded.align(\n",
    "    X_test_one_hot_encoded,\n",
    "    join=\"inner\",\n",
    "    axis=1)\n",
    "\n",
    "# Impute the missing data\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train_final)\n",
    "imputed_X_test = my_imputer.transform(X_test_final)\n",
    "\n",
    "# Show model score\n",
    "print(\"Mean Absolute Error with Imputation and One Hot Encoding:\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3\n",
    "[Learning to Use XGBoost](https://www.kaggle.com/dansbecker/learning-to-use-xgboost)\n",
    "\n",
    "This section covers XGBoost, the leading model for working with standard tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(imputed_X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean Absolute Error:16458.5521814355\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "xgb_predictions = xgb_model.predict(imputed_X_test)\n",
    "\n",
    "print(\"XGBoost Mean Absolute Error:\" + \n",
    "      str(mean_absolute_error(xgb_predictions, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean Absolute Error:16445.990261130137\n"
     ]
    }
   ],
   "source": [
    "# Tune the model by adding n_estimators and early_stopping_rounds\n",
    "xgb_model = XGBRegressor(n_estimators=88)\n",
    "xgb_model.fit(imputed_X_train, y_train, early_stopping_rounds=5,\n",
    "              eval_set=[(imputed_X_test, y_test)], verbose=False)\n",
    "\n",
    "# Predict the new model\n",
    "xgb_predictions = xgb_model.predict(imputed_X_test)\n",
    "\n",
    "print(\"XGBoost Mean Absolute Error:\" + \n",
    "      str(mean_absolute_error(xgb_predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4\n",
    "[Partial Dependence Plots](https://www.kaggle.com/dansbecker/partial-dependence-plots)\n",
    "\n",
    "This section explains how extract insights from your models using partial dependence plots, which show how each variable or predictor affects the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "front-matter": {
   "date": "2018-03-28",
   "slug": "home-prices-python-2",
   "subtitle": "",
   "title": "My code from the exercises of Level 2 of Kaggle's Learn Maching Learning series"
  },
  "hugo-jupyter": {
   "render-to": "content/post/2018/03/25/"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
