<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>My code from the exercises of Level 2 of Kaggle&#39;s Learn Machine Learning series  &middot; Hi, I&#39;m Mike the data guy</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="Python, machine learning, ">


<meta property="og:title" content="My code from the exercises of Level 2 of Kaggle&#39;s Learn Machine Learning series  &middot; Hi, I&#39;m Mike the data guy ">
<meta property="og:site_name" content="Hi, I&#39;m Mike the data guy"/>
<meta property="og:url" content="https://themichaelskolnik.com/2018/04/03/home-prices-python-2/" />
<meta property="og:locale" content="en-EN">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2018-04-03T00:00:00Z" />
<meta property="og:article:modified_time" content="2018-04-03T00:00:00Z" />

  
    
<meta property="og:article:tag" content="Python">
    
<meta property="og:article:tag" content="machine learning">
    
  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@mikeskolnik75" />
<meta name="twitter:creator" content="@mikeskolnik75" />
<meta name="twitter:title" content="My code from the exercises of Level 2 of Kaggle&#39;s Learn Machine Learning series" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="https://themichaelskolnik.com/2018/04/03/home-prices-python-2/" />
<meta name="twitter:domain" content="https://themichaelskolnik.com/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "My code from the exercises of Level 2 of Kaggle&#39;s Learn Machine Learning series",
    "author": {
      "@type": "Person",
      "name": "http://profiles.google.com/+?rel=author"
    },
    "datePublished": "2018-04-03",
    "description": "",
    "wordCount": 1757
  }
</script>



<link rel="canonical" href="https://themichaelskolnik.com/2018/04/03/home-prices-python-2/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://themichaelskolnik.com/touch-icon-144-precomposed.png">
<link href="https://themichaelskolnik.com/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.36" />

  
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="https://themichaelskolnik.com/css/font-awesome.min.css">
<link rel="stylesheet" href="https://themichaelskolnik.com/css/style.css">
<link rel="stylesheet" href="https://themichaelskolnik.com/css/highlight/default.css">

  
  
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'Your Google Analytics tracking code', 'auto');
	  ga('send', 'pageview');

	</script>

</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <a class="baselink" href="https://themichaelskolnik.com/">
  Analysis, Visualizations, and Coding

</a>

</div>

  
<div class="container topline">
  
  Data guy with SQL, Python, and R skills looking for a full-time or freelance remote position


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="https://themichaelskolnik.com/">Home</a>


  
<a href="https://themichaelskolnik.com/about/" title="About Me">About Me</a>

<a href="https://themichaelskolnik.com/post/" title="Shows a list of posts">Posts</a>

<a href="https://themichaelskolnik.com/resume/" title="My resume">Resume</a>

<a href="https://themichaelskolnik.com/tags/" title="Shows a list of tags">Tags</a>


</nav>

<div class="container nav secondary no-print">
  
<a id="contact-link-email" class="contact_link" href="mailto:mikeskolnik75@gmail.com">
  <span class="fa fa-envelope-square"></span><span>email</span></a>



<a id="contact-link-github" class="contact_link" href="https://github.com/skols">
  <span class="fa fa-github-square"></span><span>github</span></a>





<a id="contact-link-linkedin" class="contact_link" href="https://www.linkedin.com/in/mikeskolnik">
  <span class="fa fa-linkedin-square"></span><span>linkedin</span></a>







<a id="contact-link-twitter" class="contact_link" href="https://twitter.com/mikeskolnik75">
  <span class="fa fa-twitter-square"></span><span>twitter</span></a>













</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>My code from the exercises of Level 2 of Kaggle&#39;s Learn Machine Learning series
</h1>

  <div class="metas">
<time datetime="2018-04-03">3 Apr, 2018</time>


  
  &middot; Read in about 9 min
  &middot; (1757 Words)
  <br>
  
<a class="label" href="https://themichaelskolnik.com/tags/python">Python</a>

<a class="label" href="https://themichaelskolnik.com/tags/machine-learning">machine learning</a>



</div>

</header>

  <div class="container content">
  <p></p>

<h2 id="level-2-learn-maching-learning-series-on-kaggle">Level 2 <em>Learn Maching Learning</em> series on Kaggle</h2>

<p>This is the level 2 part of the <em>Learn Machine Learning</em> series on Kaggle using Python (<a href="https://www.kaggle.com/learn/machine-learning">https://www.kaggle.com/learn/machine-learning</a>). The data used is from the <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques"><em>Home Prices: Advanced Regression Techniques</em></a> competition.</p>

<p>Like the post for level 1, this post will show the section name, my code from the corresponding section for the instructions under <strong>Your Turn</strong>, and some brief notes on what is taught in each section.</p>

<p>First I&rsquo;ll run the necessary code from before and add a new function, score_dataset.</p>

<pre><code class="language-python"># Import the necessary libraries
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor


# Save filepath to variable
training_data_filepath = &quot;C:/Development/Kaggle/House Prices - Advanced \
Regression Techniques/train.csv&quot;

# Read the data and store in a dataframe called training_set
training_set = pd.read_csv(training_data_filepath)

# Select the target variable and call it y
y = training_set.SalePrice

# Create the dataframe with only numeric predictors, dropping Id and SalePrice
X = training_set.drop([&quot;Id&quot;, &quot;SalePrice&quot;], axis=1)\
        .select_dtypes(exclude=[&quot;object&quot;])

# Split data into training and validation data, for both predictors and
# target.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,
                                                 train_size=0.7,
                                                 test_size=0.3)

def score_dataset(X_train, X_test, y_train, y_test):
    model = RandomForestRegressor()
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    return mean_absolute_error(y_test, preds)
</code></pre>

<h3 id="section-1">Section 1</h3>

<p><a href="https://www.kaggle.com/dansbecker/handling-missing-values">Handling Missing Values</a></p>

<p>This section teaches multiple approaches for dealing with missing data fields.</p>

<pre><code class="language-python"># Detect which columns have missing values
print(X.isnull().sum())
</code></pre>

<pre><code>MSSubClass         0
LotFrontage      259
LotArea            0
OverallQual        0
OverallCond        0
YearBuilt          0
YearRemodAdd       0
MasVnrArea         8
BsmtFinSF1         0
BsmtFinSF2         0
BsmtUnfSF          0
TotalBsmtSF        0
1stFlrSF           0
2ndFlrSF           0
LowQualFinSF       0
GrLivArea          0
BsmtFullBath       0
BsmtHalfBath       0
FullBath           0
HalfBath           0
BedroomAbvGr       0
KitchenAbvGr       0
TotRmsAbvGrd       0
Fireplaces         0
GarageYrBlt       81
GarageCars         0
GarageArea         0
WoodDeckSF         0
OpenPorchSF        0
EnclosedPorch      0
3SsnPorch          0
ScreenPorch        0
PoolArea           0
MiscVal            0
MoSold             0
YrSold             0
dtype: int64
</code></pre>

<pre><code class="language-python"># Get model score from dropping columns with missing values
cols_with_missing = [col for col in X_train.columns
                     if X_train[col].isnull().any()]
reduced_X_train = X_train.drop(cols_with_missing, axis=1)
reduced_X_test = X_test.drop(cols_with_missing, axis=1)

print(&quot;Mean Absolute Error from dropping columns with missing values:&quot;)
print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))
</code></pre>

<pre><code>Mean Absolute Error from dropping columns with missing values:
17760.797792998477
</code></pre>

<pre><code class="language-python"># Get model score from Imputation
from sklearn.preprocessing import Imputer


my_imputer = Imputer()
imputed_X_train = my_imputer.fit_transform(X_train)
imputed_X_test = my_imputer.transform(X_test)

# &quot;fit_transform&quot; is the training step. It &quot;learns&quot; based upon the training set data.
# &quot;transform&quot; uses the newly trained model to make predictions on the &quot;test set&quot;
# (a.k.a. &quot;validation set&quot; in the the first tutorial).

print(&quot;Mean Absolute Error from Imputation:&quot;)
print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))
</code></pre>

<pre><code>Mean Absolute Error from Imputation:
18445.657762557075
</code></pre>

<pre><code class="language-python"># Get model score from Imputation with extra columns showing what was imputed
imputed_X_train_plus = X_train.copy()
imputed_X_test_plus = X_test.copy()

cols_with_missing = (col for col in X_train.columns
                    if X_train[col].isnull().any())

for col in cols_with_missing:
    imputed_X_train_plus[col + &quot;_was_missing&quot;] = imputed_X_train_plus[col].isnull()
    imputed_X_test_plus[col + &quot;_was_missing&quot;] = imputed_X_test_plus[col].isnull()

# Imputation
imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)
imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)

print(&quot;Mean Absolute Error from Imputation while tracking what was imputed:&quot;)
print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))
</code></pre>

<pre><code>Mean Absolute Error from Imputation while tracking what was imputed:
18617.85372907154
</code></pre>

<h3 id="section-2">Section 2</h3>

<p><a href="https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding">Using Categorical Data with One Hot Encoding</a></p>

<p>In this section you learn how to handle categorical data by using one hot encoding, which creates new columns for each value in the categorical field. The new columns will have either a 1 or 0 in them.</p>

<pre><code class="language-python"># Using cardinality as a way to select categorical data. &quot;cardinality&quot; means
# the number of unique values in a column.
candidate_train_predictors = training_set.drop([&quot;Id&quot;, &quot;SalePrice&quot;], axis=1)

low_cardinality_cols = [cname for cname in candidate_train_predictors if
                       candidate_train_predictors[cname].nunique() &lt; 10 and
                       candidate_train_predictors[cname].dtype == &quot;object&quot;]
numeric_cols = [cname for cname in candidate_train_predictors if
               candidate_train_predictors[cname].dtype in
                [&quot;int64&quot;, &quot;float64&quot;]]

my_cols = low_cardinality_cols + numeric_cols
X = candidate_train_predictors[my_cols]

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    random_state=42,
                                                    train_size=0.7,
                                                    test_size=0.3)

# Using one hot encoding the categorical variables
X_train_one_hot_encoded = pd.get_dummies(X_train)
X_test_one_hot_encoded = pd.get_dummies(X_test)

# Make sure the columns show up in the same order by using the align method
# &quot;join='inner'&quot; is like an inner join in SQL, keeping only the columns in
# both datasets
X_train_final, X_test_final = X_train_one_hot_encoded.align(
    X_test_one_hot_encoded,
    join=&quot;inner&quot;,
    axis=1)

# Impute the missing data
my_imputer = Imputer()
imputed_X_train = my_imputer.fit_transform(X_train_final)
imputed_X_test = my_imputer.transform(X_test_final)

# Show model score
print(&quot;Mean Absolute Error with Imputation and One Hot Encoding:&quot;)
print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))
</code></pre>

<pre><code>Mean Absolute Error with Imputation and One Hot Encoding:
19233.457077625568
</code></pre>

<h3 id="section-3">Section 3</h3>

<p><a href="https://www.kaggle.com/dansbecker/learning-to-use-xgboost">Learning to Use XGBoost</a></p>

<p>This section covers XGBoost, the leading model for working with standard tabular data.</p>

<pre><code class="language-python"># Import XGBoost
from xgboost import XGBRegressor


xgb_model = XGBRegressor()
xgb_model.fit(imputed_X_train, y_train, verbose=False)
</code></pre>

<pre><code>c:\users\micha\anaconda3\lib\site-packages\sklearn\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  &quot;This module will be removed in 0.20.&quot;, DeprecationWarning)





XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,
       learning_rate=0.1, max_delta_step=0, max_depth=3,
       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
       objective='reg:linear', reg_alpha=0, reg_lambda=1,
       scale_pos_weight=1, seed=0, silent=True, subsample=1)
</code></pre>

<pre><code class="language-python"># Make predictions
xgb_predictions = xgb_model.predict(imputed_X_test)

print(&quot;XGBoost Mean Absolute Error:&quot; + 
      str(mean_absolute_error(xgb_predictions, y_test)))
</code></pre>

<pre><code>XGBoost Mean Absolute Error:16458.5521814355
</code></pre>

<pre><code class="language-python"># Tune the model by adding n_estimators and early_stopping_rounds
xgb_model = XGBRegressor(n_estimators=1000)
xgb_model.fit(imputed_X_train, y_train, early_stopping_rounds=20,
              eval_set=[(imputed_X_test, y_test)], verbose=False)

print(&quot;Model best_score: &quot; + str(xgb_model.best_score))
print(&quot;Model best_iteration: &quot; + str(xgb_model.best_iteration))
print(&quot;Model best_ntree_limit:&quot; + str(xgb_model.best_ntree_limit))
</code></pre>

<pre><code>Model best_score: 27433.203125
Model best_iteration: 87
Model best_ntree_limit:88
</code></pre>

<pre><code class="language-python"># Run the model with the best_iteration and best_ntree_limit, then predict
xgb_model_bi = XGBRegressor(n_estimators=87)
xgb_model_bi.fit(imputed_X_train, y_train,
              eval_set=[(imputed_X_test, y_test)], verbose=False)

xgb_predictions_bi = xgb_model_bi.predict(imputed_X_test)

xgb_model_bnl = XGBRegressor(n_estimators=88)
xgb_model_bnl.fit(imputed_X_train, y_train,
              eval_set=[(imputed_X_test, y_test)], verbose=False)

xgb_predictions_bnl = xgb_model_bnl.predict(imputed_X_test)

print(&quot;XGBoost Mean Absolute Error with best_iteration: &quot; + 
      str(mean_absolute_error(xgb_predictions_bi, y_test)))
print(&quot;XGBoost Mean Absolute Error with best_ntree_limit: &quot; + 
      str(mean_absolute_error(xgb_predictions_bnl, y_test)))
</code></pre>

<pre><code>XGBoost Mean Absolute Error with best_iteration: 16450.916060216896
XGBoost Mean Absolute Error with best_ntree_limit: 16445.990261130137
</code></pre>

<h3 id="section-4">Section 4</h3>

<p><a href="https://www.kaggle.com/dansbecker/partial-dependence-plots">Partial Dependence Plots</a></p>

<p>This section explains how extract insights from your models using partial dependence plots, which show how each variable or predictor affects the model&rsquo;s predictions.</p>

<pre><code class="language-python"># Create a function that returns X and y
def get_data(df, cols_to_use):
    y = df.SalePrice
    X = df[cols_to_use]
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        random_state=42)
    my_imputer = Imputer()
    imputed_X_train = my_imputer.fit_transform(X_train)
    imputed_X_test = my_imputer.fit_transform(X_test)
    return imputed_X_train, imputed_X_test, y_train, y_test


# Create a list of three predictor variables
predictors = [&quot;LotArea&quot;, &quot;YearBuilt&quot;, &quot;TotRmsAbvGrd&quot;]

# Call the function
X_train, X_test, y_train, y_test = get_data(training_set, predictors)
</code></pre>

<pre><code class="language-python"># Import libraries
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence
%matplotlib inline


my_model = GradientBoostingRegressor()
my_model.fit(X_train, y_train)

# Hypotheses for each predictor
# As LotArea increases, SalePrice increases
# As YearBuilt increases, SalePrice increases
# As TotRmsAbvGrd increases, SalePrice increases
my_plots = plot_partial_dependence(my_model,
                                  features=[0, 1, 2],
                                  X=X_train,
                                  feature_names=[&quot;LotArea&quot;,
                                                 &quot;YearBuilt&quot;,
                                                 &quot;TotRmsAbvGrd&quot;],
                                  grid_resolution=5)
</code></pre>


<figure >
    
        <img src="/post/2018/04/01/output_16_0.png" />
    
    
</figure>


<p>After a certain LotArea, SalePrice begins to decrease. With YearBuilt, there is a leveling off for about 20 years.</p>

<h3 id="section-5">Section 5</h3>

<p><a href="https://www.kaggle.com/dansbecker/pipelines">Pipelines</a></p>

<p>This section covers pipelines and how they make your code cleaner and more professional.</p>

<pre><code class="language-python"># Create the data with one hot encoding
candidate_train_predictors = training_set.drop([&quot;Id&quot;, &quot;SalePrice&quot;], axis=1)

low_cardinality_cols = [cname for cname in candidate_train_predictors if
                       candidate_train_predictors[cname].nunique() &lt; 10 and
                       candidate_train_predictors[cname].dtype == &quot;object&quot;]
numeric_cols = [cname for cname in candidate_train_predictors if
               candidate_train_predictors[cname].dtype in
                [&quot;int64&quot;, &quot;float64&quot;]]

my_cols = low_cardinality_cols + numeric_cols
X = candidate_train_predictors[my_cols]
y = training_set.SalePrice

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    random_state=42,
                                                    train_size=0.7,
                                                    test_size=0.3)

# Using one hot encoding the categorical variables
X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

# Make sure the columns show up in the same order by using the align method
# &quot;join='inner'&quot; is like an inner join in SQL, keeping only the columns in
# both datasets
X_train, X_test = X_train_one_hot_encoded.align(
    X_test_one_hot_encoded,
    join=&quot;inner&quot;,
    axis=1)
</code></pre>

<pre><code class="language-python"># Import library
from sklearn.pipeline import make_pipeline


# Create the pipeline
my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())

# Fit and predict using the pipeline
my_pipeline.fit(X_train, y_train)
predictions = my_pipeline.predict(X_test)

# Print MAE
print(&quot;Mean Absolue Error: &quot; + str(mean_absolute_error(predictions, y_test)))
</code></pre>

<pre><code>Mean Absolue Error: 18159.600684931505
</code></pre>

<h3 id="section-6">Section 6</h3>

<p><a href="https://www.kaggle.com/dansbecker/cross-validation">Cross-Validation</a></p>

<p>This section covers cross-validation, which gives you a more reliable measure of your model&rsquo;s quality.</p>

<pre><code class="language-python"># Create the data and make the pipeline
y = training_set.SalePrice

predictors = [&quot;LotArea&quot;, &quot;YearBuilt&quot;, &quot;1stFlrSF&quot;, &quot;2ndFlrSF&quot;, &quot;FullBath&quot;,
              &quot;BedroomAbvGr&quot;, &quot;TotRmsAbvGrd&quot;]

X = training_set[predictors]

my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())
</code></pre>

<pre><code class="language-python"># Import library
from sklearn.model_selection import cross_val_score


# Get the cross-validation scores
scores = cross_val_score(my_pipeline, X, y,
                         scoring=&quot;neg_mean_absolute_error&quot;)

print(scores)
</code></pre>

<pre><code>[-23619.64847463 -24044.31307324 -25411.42688615]
</code></pre>

<pre><code class="language-python"># Print the average of the scores to get a single measure
print(&quot;Mean Absolute Error: {0}&quot;.format(-1 * scores.mean()))
</code></pre>

<pre><code>Mean Absolute Error: 24358.4628113363
</code></pre>

<pre><code class="language-python"># Remove YearBuilt and rerun
predictors = [&quot;LotArea&quot;, &quot;1stFlrSF&quot;, &quot;2ndFlrSF&quot;, &quot;FullBath&quot;,
              &quot;BedroomAbvGr&quot;, &quot;TotRmsAbvGrd&quot;]

X = training_set[predictors]

scores = cross_val_score(my_pipeline, X, y,
                         scoring=&quot;neg_mean_absolute_error&quot;)

print(scores)
print(&quot;Mean Absolute Error: {0}&quot;.format(-1 * scores.mean()))
</code></pre>

<pre><code>[-31607.33867214 -28752.41276523 -30952.68010288]
Mean Absolute Error: 30437.477180084108
</code></pre>

<pre><code class="language-python"># Use all numeric fields and rerun
X = training_set.drop([&quot;Id&quot;, &quot;SalePrice&quot;], axis=1)\
        .select_dtypes(exclude=[&quot;object&quot;])

scores = cross_val_score(my_pipeline, X, y,
                         scoring=&quot;neg_mean_absolute_error&quot;)

print(scores)
print(&quot;Mean Absolute Error: {0}&quot;.format(-1 * scores.mean()))
</code></pre>

<pre><code>[-19261.77241615 -18868.2174538  -20048.12942387]
Mean Absolute Error: 19392.70643127347
</code></pre>

<h3 id="section-7">Section 7</h3>

<p><a href="https://www.kaggle.com/dansbecker/data-leakage">Data Leakage</a></p>

<p>This section covers how to identify and avoid data leakage, one of the most common and costly mistakes in machine learning.</p>

<pre><code class="language-python"># Create the data, make the pipeline, and get the accuracy cross-validation
# score
y = training_set.SalePrice

predictors = [&quot;LotArea&quot;, &quot;YearBuilt&quot;, &quot;1stFlrSF&quot;, &quot;2ndFlrSF&quot;, &quot;FullBath&quot;,
              &quot;BedroomAbvGr&quot;, &quot;TotRmsAbvGrd&quot;]

X = training_set.drop([&quot;Id&quot;, &quot;SalePrice&quot;], axis=1)\
        .select_dtypes(exclude=[&quot;object&quot;])

my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())

scores = cross_val_score(my_pipeline, X, y,
                         scoring=&quot;neg_mean_absolute_error&quot;)

print(scores)
print(&quot;Mean Absolute Error: {0}&quot;.format(-1 * scores.mean()))
</code></pre>

<pre><code>[-19986.84188912 -18771.71396304 -19436.84814815]
Mean Absolute Error: 19398.468000101402
</code></pre>

<pre><code class="language-python"># Identify some potential leaks and run after removing them
# Most datasets from Kaggle competitions don't have variables that cause
# data leakage, but I'm choosing some that I think might.
potential_leaks = [&quot;MiscVal&quot;, &quot;MoSold&quot;, &quot;YrSold&quot;]

X2 = X.drop(potential_leaks, axis=1)

scores = cross_val_score(my_pipeline, X2, y,
                         scoring=&quot;neg_mean_absolute_error&quot;)

print(scores)
print(&quot;Mean Absolute Error with leaks removed: {0}&quot;\
      .format(-1 * scores.mean()))
</code></pre>

<pre><code>[-18974.61902806 -20104.03839836 -20104.8617284 ]
Mean Absolute Error with leaks removed: 19727.839718271774
</code></pre>

<p>The MAE is higher after the leaks are removed but that makes sense. That doesn&rsquo;t mean those variables were leaks, but they did help with prediction. They may not have been that valid though.</p>

<p>This wraps up the level 2 part of the <em>Learn Machine Learning</em> series on Kaggle. The way this level is structured, it doesn&rsquo;t make sense to have one place for all the code; it makes more sense to me to keep it all separate.
Also, it took me a little over a week to start and finish, which I don&rsquo;t think is too bad considering I had aimed for a week or less. I now know that a week is an achievable goal for posts like this. Some may take less, others more. Like I said in the level 1 post, next I&rsquo;m going to recreate these tutorials in R. Those don&rsquo;t exist on Kaggle, so I&rsquo;m looking forward to the challenge!</p>
</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="https://themichaelskolnik.com/2018/03/25/home-prices-python/" title="My code from the exercises of Level 1 of Kaggle&#39;s Learn Machine Learning series">
      Previous
    </a>
    

    
    <a class="next" href="https://themichaelskolnik.com/2018/04/19/home-prices-r1/" title="My R code from the Level 1 exercises of Kaggle&#39;s Learn Machine Learning series">
      Next
    </a>
    

  


</div>

  <div class="container comments">
  <h2>Comments</h2>
  
<div id="disqus_thread"></div>
<script type="text/javascript">
  (function() {
    
    
    if (window.location.hostname == "localhost")
      return;

    var dsq = document.createElement('script'); dsq.async = true; dsq.type = 'text/javascript';
    dsq.src = '//themichaelskolnik-com.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


</div>

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  code with <i class='fa fa-heart'></i>


</div>


  
<div class="container copyright">
  
  &copy; 2018 Michael Skolnik


</div>


</div>

</footer>

    </main>
    
<script type="text/javascript">
  (function() {
    
    
    if (window.location.hostname == "localhost")
      return;

    var dsq = document.createElement('script'); dsq.async = true; dsq.type = 'text/javascript';
    dsq.src = '//themichaelskolnik-com.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>



<script src="https://themichaelskolnik.com/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


    
  </body>
</html>

